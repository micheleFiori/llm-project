{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e23b209ad1e8d4ce",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T07:58:02.644677200Z",
     "start_time": "2023-10-27T07:57:50.995919700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohce\\llm-project\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "from context_descriptor import ContextDescriptor\n",
    "from prompt_generator import PromptGenerator\n",
    "from model_request_handler import ModelRequestHandler\n",
    "from open_ai_request_handler import OpenAiRequestHandler\n",
    "import numpy as np\n",
    "from examples_manager import Examples_manager\n",
    "from sklearn.metrics import classification_report\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24de087aff90f16b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T07:58:06.234600Z",
     "start_time": "2023-10-27T07:58:06.165581900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OPEN_AI_API_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c99adbbe10c421b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T07:58:06.498576900Z",
     "start_time": "2023-10-27T07:58:06.432579400Z"
    }
   },
   "outputs": [],
   "source": [
    "DATASET = \"DOMINO\"\n",
    "if DATASET not in [\"DOMINO\", \"EXTRASENSORY\"]:\n",
    "    raise ValueError(\"DATASET name must be 'DOMINO' or 'EXTRASENSORY'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa24b24792f4e1d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T07:58:07.022588400Z",
     "start_time": "2023-10-27T07:58:06.886582300Z"
    }
   },
   "outputs": [],
   "source": [
    "extrasensory_classes = ['BICICLYNG', 'LAYING_DOWN', 'MOVING_BY_CAR', 'ON_TRANSPORT', 'SITTING', 'STANDING', 'WALKING']\n",
    "domino_classes = ['BRUSHING_TEETH', 'CYCLING', 'ELEVATOR_DOWN', 'ELEVATOR_UP', 'LYING', 'MOVING_BY_CAR', 'RUNNING', 'SITTING', 'SITTING_ON_TRANSPORT', 'STAIRS_DOWN', 'STAIRS_UP', 'STANDING', 'STANDING_ON_TRANSPORT', 'WALKING']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "773d17aa4c791892",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T07:58:08.011578900Z",
     "start_time": "2023-10-27T07:58:07.921577800Z"
    }
   },
   "outputs": [],
   "source": [
    "if DATASET == \"DOMINO\":\n",
    "    CLASSES = domino_classes\n",
    "elif DATASET == \"EXTRASENSORY\":\n",
    "    CLASSES = extrasensory_classes\n",
    "else:\n",
    "    CLASSES = None\n",
    "    raise ValueError(\"DATASET name must be 'DOMINO' or 'EXTRASENSORY'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b13ddfd26147de",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b0df3761f738f38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T07:58:13.173160700Z",
     "start_time": "2023-10-27T07:58:13.000168300Z"
    }
   },
   "outputs": [],
   "source": [
    "if DATASET == \"DOMINO\":\n",
    "    with open(\"datasets/domino/domino_unique_contexts.pkl\", \"rb\") as file:\n",
    "        unique_contexts = pkl.load(file)\n",
    "    with open(\"datasets/domino/domino_unique_contexts_ontology_consistencies.pkl\", \"rb\") as file:\n",
    "        unique_contexts_ontology_consistencies = pkl.load(file)\n",
    "    with open(\"datasets/domino/domino_unique_contexts_labels.pkl\", \"rb\") as file:\n",
    "        unique_contexts_labels = pkl.load(file)\n",
    "\n",
    "elif DATASET == \"EXTRASENSORY\":\n",
    "    with open(\"datasets/extrasensory/extrasensory_unique_contexts.pkl\", \"rb\") as file:\n",
    "        unique_contexts = pkl.load(file)\n",
    "    with open(\"datasets/extrasensory/extrasensory_unique_contexts_labels.pkl\", \"rb\") as file:\n",
    "        unique_contexts_labels = pkl.load(file)\n",
    "    with open(\"datasets/extrasensory/extrasensory_unique_contexts_ontology_consistencies.pkl\", \"rb\") as file:\n",
    "        unique_contexts_ontology_consistencies = pkl.load(file)\n",
    "        \n",
    "else:\n",
    "    raise ValueError(\"DATASET name must be 'DOMINO' or 'EXTRASENSORY'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ecfded74ee4b8e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define system message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "404b020aea980bb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T07:58:14.433204Z",
     "start_time": "2023-10-27T07:58:14.378203600Z"
    }
   },
   "outputs": [],
   "source": [
    "system_message = f'''You are a domain expert in charge of checking which human activities are suitable with respect to the current surrounding context of a user. You will receive as input a description of the surrounding context of the user. You have to determine which are the activities that are suitable in the given context. This is the list of the possible user activities that you have to consider: {', '.join(CLASSES)}. Accomplish the task by following the steps below:Step 1: Analyze the context.Step 2: Determine which activities are likely in the given context, by analyzing each activity. Apply the open-world assumption: anything that is not explicit in the surrounding context of the user may be possible. Do not exclude activities for which you cannot determine their likelihood or the ones for which there are no specific information about objects and vehicles that the user is using.Step 3: Provide the result in the form of a list.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a0dba073a55bfd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T07:58:15.266544800Z",
     "start_time": "2023-10-27T07:58:15.240564600Z"
    }
   },
   "outputs": [],
   "source": [
    "#Think about the \"answer and step by step analyze each activity.\n",
    "#system_message += \"In the following i'll give you some examples of contexts that you can receive and the answer \" \\\n",
    "#                  \"that i would expect: \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e4c3d280c439ea",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Initialize helper classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b47b2049f579e102",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T07:58:19.397204Z",
     "start_time": "2023-10-27T07:58:19.362170900Z"
    }
   },
   "outputs": [],
   "source": [
    "contest_descriptor = ContextDescriptor()\n",
    "prompt_generator = PromptGenerator(system_message)\n",
    "#request_handler = ModelRequestHandler()\n",
    "request_handler = OpenAiRequestHandler(api_key=OPEN_AI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b53742f1deddf6b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Initialize example manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abb45f3dbdbb037a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T07:58:24.022851Z",
     "start_time": "2023-10-27T07:58:21.941850700Z"
    }
   },
   "outputs": [],
   "source": [
    "if DATASET == \"DOMINO\":\n",
    "    example_manager = Examples_manager(\"domino_examples.csv\")\n",
    "elif DATASET == \"EXTRASENSORY\":\n",
    "    example_manager = Examples_manager(\"extrasensory_examples.csv\")\n",
    "else:\n",
    "    example_manager = None\n",
    "    raise ValueError(\"DATASET name must be 'DOMINO' or 'EXTRASENSORY'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9685c917be20d219",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Choose context desciption function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7535f1e5c1376695",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T07:58:25.235539500Z",
     "start_time": "2023-10-27T07:58:25.199542300Z"
    }
   },
   "outputs": [],
   "source": [
    "if DATASET == \"DOMINO\":\n",
    "    create_context_description = lambda context_vector: contest_descriptor.create_domino_context_description(context_vector)\n",
    "elif DATASET == \"EXTRASENSORY\":\n",
    "    create_context_description = lambda context_vector: contest_descriptor.create_extrasensory_context_description(context_vector)\n",
    "else:\n",
    "    create_context_description = None\n",
    "    raise ValueError(\"DATASET name must be 'DOMINO' or 'EXTRASENSORY'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572581ba85558ab7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Make the api calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7fd20f31ef70dc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T08:05:20.440767800Z",
     "start_time": "2023-10-27T07:58:30.534272900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  1\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0]\n",
      "['Indoor', 'Barbershop', 'NullSpeed', 'Misty', 'NotOnPublicTransportationRoute', 'NullHeightVariation', 'LowAudioLevel']\n",
      "CURRENT:   In the last 4 seconds the user Bob was in an Indoor environment at the Barbershop, where he was not moving, experiencing Misty weather, not following/close to a public transportation route, and neither going up nor down.\n",
      "[{'role': 'system', 'content': 'You are a domain expert in charge of checking which human activities are suitable with respect to the current surrounding context of a user. You will receive as input a description of the surrounding context of the user. You have to determine which are the activities that are suitable in the given context. This is the list of the possible user activities that you have to consider: BRUSHING_TEETH, CYCLING, ELEVATOR_DOWN, ELEVATOR_UP, LYING, MOVING_BY_CAR, RUNNING, SITTING, SITTING_ON_TRANSPORT, STAIRS_DOWN, STAIRS_UP, STANDING, STANDING_ON_TRANSPORT, WALKING. Accomplish the task by following the steps below:Step 1: Analyze the context.Step 2: Determine which activities are likely in the given context, by analyzing each activity. Apply the open-world assumption: anything that is not explicit in the surrounding context of the user may be possible. Do not exclude activities for which you cannot determine their likelihood or the ones for which there are no specific information about objects and vehicles that the user is using.Step 3: Provide the result in the form of a list.'}, {'role': 'user', 'content': \"Context: the user was indoor, at the workplace, where he was not moving, experiencing sunny weather, not following/close a public transportation route, and neither going up nor down. Likely activities: ['BRUSHING_TEETH', 'LYING', 'SITTING', 'STANDING']. Context: the user was indoor, at home, where he was not moving, experiencing sunny weather, not following/close a public transportation route, and neither going up nor down. Likely activities: ['BRUSHING_TEETH', 'CYCLING', 'LYING', 'RUNNING', 'SITTING', 'STANDING']. Context: the user was indoor, at the mall, not moving, experiencing sunny weather, not following/close a public transportation route, and neither going up nor down. Likely activities: ['LYING', 'SITTING', 'STANDING']. . Context:  In the last 4 seconds the user Bob was in an Indoor environment at the Barbershop, where he was not moving, experiencing Misty weather, not following/close to a public transportation route, and neither going up nor down.. Likely activities: \"}]\n",
      "---------------------------------------------------------------------------------\n",
      "Index:  2\n",
      "[1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0]\n",
      "['Indoor', 'Home', 'NullSpeed', 'Cloudy', 'NotOnPublicTransportationRoute', 'NullHeightVariation', 'MediumAudioLevel']\n",
      "CURRENT:   In the last 4 seconds the user Bob was in an Indoor environment at Home, where he was not moving, experiencing Cloudy weather, not following/close to a public transportation route, and neither going up nor down.\n",
      "[{'role': 'system', 'content': 'You are a domain expert in charge of checking which human activities are suitable with respect to the current surrounding context of a user. You will receive as input a description of the surrounding context of the user. You have to determine which are the activities that are suitable in the given context. This is the list of the possible user activities that you have to consider: BRUSHING_TEETH, CYCLING, ELEVATOR_DOWN, ELEVATOR_UP, LYING, MOVING_BY_CAR, RUNNING, SITTING, SITTING_ON_TRANSPORT, STAIRS_DOWN, STAIRS_UP, STANDING, STANDING_ON_TRANSPORT, WALKING. Accomplish the task by following the steps below:Step 1: Analyze the context.Step 2: Determine which activities are likely in the given context, by analyzing each activity. Apply the open-world assumption: anything that is not explicit in the surrounding context of the user may be possible. Do not exclude activities for which you cannot determine their likelihood or the ones for which there are no specific information about objects and vehicles that the user is using.Step 3: Provide the result in the form of a list.'}, {'role': 'user', 'content': \"Context: the user was indoor, at home, where he was not moving, experiencing sunny weather, not following/close a public transportation route, and neither going up nor down. Likely activities: ['BRUSHING_TEETH', 'CYCLING', 'LYING', 'RUNNING', 'SITTING', 'STANDING']. Context: the user was indoor, at the workplace, where he was not moving, experiencing sunny weather, not following/close a public transportation route, and neither going up nor down. Likely activities: ['BRUSHING_TEETH', 'LYING', 'SITTING', 'STANDING']. Context: the user was indoor, at a station, not moving, experiencing sunny weather, following/close a public trasportation route, and neither going up nor down. Likely activities: ['LYING', 'SITTING', 'STANDING']. . Context:  In the last 4 seconds the user Bob was in an Indoor environment at Home, where he was not moving, experiencing Cloudy weather, not following/close to a public transportation route, and neither going up nor down.. Likely activities: \"}]\n",
      "---------------------------------------------------------------------------------\n",
      "Index:  3\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0]\n",
      "['Indoor', 'Bank', 'NullSpeed', 'Drizzly', 'NotOnPublicTransportationRoute', 'NullHeightVariation', 'LowAudioLevel']\n",
      "CURRENT:   In the last 4 seconds the user Bob was in an Indoor environment at the Bank, where he was not moving, experiencing Drizzly weather, not following/close to a public transportation route, and neither going up nor down.\n",
      "[{'role': 'system', 'content': 'You are a domain expert in charge of checking which human activities are suitable with respect to the current surrounding context of a user. You will receive as input a description of the surrounding context of the user. You have to determine which are the activities that are suitable in the given context. This is the list of the possible user activities that you have to consider: BRUSHING_TEETH, CYCLING, ELEVATOR_DOWN, ELEVATOR_UP, LYING, MOVING_BY_CAR, RUNNING, SITTING, SITTING_ON_TRANSPORT, STAIRS_DOWN, STAIRS_UP, STANDING, STANDING_ON_TRANSPORT, WALKING. Accomplish the task by following the steps below:Step 1: Analyze the context.Step 2: Determine which activities are likely in the given context, by analyzing each activity. Apply the open-world assumption: anything that is not explicit in the surrounding context of the user may be possible. Do not exclude activities for which you cannot determine their likelihood or the ones for which there are no specific information about objects and vehicles that the user is using.Step 3: Provide the result in the form of a list.'}, {'role': 'user', 'content': \"Context: the user was indoor, at home, where he was not moving, experiencing sunny weather, not following/close a public transportation route, and neither going up nor down. Likely activities: ['BRUSHING_TEETH', 'CYCLING', 'LYING', 'RUNNING', 'SITTING', 'STANDING']. Context: the user was indoor, at the restaurant,where he was moving/travelling at a speed between 1 and 4 km/h, experiencing drizzly weather, not following/close/close to a public transportation route, and neither going up nor down. Likely activities: ['WALKING']. Context: the user was indoor, at the workplace, where he was not moving, experiencing sunny weather, not following/close a public transportation route, and neither going up nor down. Likely activities: ['BRUSHING_TEETH', 'LYING', 'SITTING', 'STANDING']. . Context:  In the last 4 seconds the user Bob was in an Indoor environment at the Bank, where he was not moving, experiencing Drizzly weather, not following/close to a public transportation route, and neither going up nor down.. Likely activities: \"}]\n",
      "---------------------------------------------------------------------------------\n",
      "Index:  4\n",
      "[1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1]\n",
      "['Indoor', 'Mall', 'NullSpeed', 'Rainy', 'NotOnPublicTransportationRoute', 'NullHeightVariation', 'HighAudioLevel']\n",
      "CURRENT:   In the last 4 seconds the user Bob was in an Indoor environment at the Mall, where he was not moving, experiencing Rainy weather, not following/close to a public transportation route, and neither going up nor down.\n",
      "[{'role': 'system', 'content': 'You are a domain expert in charge of checking which human activities are suitable with respect to the current surrounding context of a user. You will receive as input a description of the surrounding context of the user. You have to determine which are the activities that are suitable in the given context. This is the list of the possible user activities that you have to consider: BRUSHING_TEETH, CYCLING, ELEVATOR_DOWN, ELEVATOR_UP, LYING, MOVING_BY_CAR, RUNNING, SITTING, SITTING_ON_TRANSPORT, STAIRS_DOWN, STAIRS_UP, STANDING, STANDING_ON_TRANSPORT, WALKING. Accomplish the task by following the steps below:Step 1: Analyze the context.Step 2: Determine which activities are likely in the given context, by analyzing each activity. Apply the open-world assumption: anything that is not explicit in the surrounding context of the user may be possible. Do not exclude activities for which you cannot determine their likelihood or the ones for which there are no specific information about objects and vehicles that the user is using.Step 3: Provide the result in the form of a list.'}, {'role': 'user', 'content': \"Context: the user was indoor, at the mall, not moving, experiencing sunny weather, not following/close a public transportation route, and neither going up nor down. Likely activities: ['LYING', 'SITTING', 'STANDING']. Context: the user was indoor, at home, where he was not moving, experiencing sunny weather, not following/close a public transportation route, and neither going up nor down. Likely activities: ['BRUSHING_TEETH', 'CYCLING', 'LYING', 'RUNNING', 'SITTING', 'STANDING']. Context: the user was indoor, at the workplace, where he was not moving, experiencing sunny weather, not following/close a public transportation route, and neither going up nor down. Likely activities: ['BRUSHING_TEETH', 'LYING', 'SITTING', 'STANDING']. . Context:  In the last 4 seconds the user Bob was in an Indoor environment at the Mall, where he was not moving, experiencing Rainy weather, not following/close to a public transportation route, and neither going up nor down.. Likely activities: \"}]\n",
      "---------------------------------------------------------------------------------\n",
      "Index:  5\n",
      "[1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0]\n",
      "['Indoor', 'Bar', 'NullSpeed', 'Misty', 'NotOnPublicTransportationRoute', 'NullHeightVariation', 'MediumAudioLevel']\n",
      "CURRENT:   In the last 4 seconds the user Bob was in an Indoor environment at the Bar, where he was not moving, experiencing Misty weather, not following/close to a public transportation route, and neither going up nor down.\n",
      "[{'role': 'system', 'content': 'You are a domain expert in charge of checking which human activities are suitable with respect to the current surrounding context of a user. You will receive as input a description of the surrounding context of the user. You have to determine which are the activities that are suitable in the given context. This is the list of the possible user activities that you have to consider: BRUSHING_TEETH, CYCLING, ELEVATOR_DOWN, ELEVATOR_UP, LYING, MOVING_BY_CAR, RUNNING, SITTING, SITTING_ON_TRANSPORT, STAIRS_DOWN, STAIRS_UP, STANDING, STANDING_ON_TRANSPORT, WALKING. Accomplish the task by following the steps below:Step 1: Analyze the context.Step 2: Determine which activities are likely in the given context, by analyzing each activity. Apply the open-world assumption: anything that is not explicit in the surrounding context of the user may be possible. Do not exclude activities for which you cannot determine their likelihood or the ones for which there are no specific information about objects and vehicles that the user is using.Step 3: Provide the result in the form of a list.'}, {'role': 'user', 'content': \"Context: the user was indoor, at home, where he was not moving, experiencing sunny weather, not following/close a public transportation route, and neither going up nor down. Likely activities: ['BRUSHING_TEETH', 'CYCLING', 'LYING', 'RUNNING', 'SITTING', 'STANDING']. Context: the user was indoor, at the workplace, where he was not moving, experiencing sunny weather, not following/close a public transportation route, and neither going up nor down. Likely activities: ['BRUSHING_TEETH', 'LYING', 'SITTING', 'STANDING']. Context: the user was indoor, at the gym, not moving and experiencing cloudy weather. He was not following/close to a public transportation route, and he was neither going up nor down. Likely activities: ['CYCLING', 'RUNNING', 'LYING', 'SITTING','STANDING']. . Context:  In the last 4 seconds the user Bob was in an Indoor environment at the Bar, where he was not moving, experiencing Misty weather, not following/close to a public transportation route, and neither going up nor down.. Likely activities: \"}]\n",
      "---------------------------------------------------------------------------------\n",
      "Index:  6\n",
      "[1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0]\n",
      "['Indoor', 'Mall', 'NullSpeed', 'Drizzly', 'NotOnPublicTransportationRoute', 'NullHeightVariation', 'MediumAudioLevel']\n",
      "CURRENT:   In the last 4 seconds the user Bob was in an Indoor environment at the Mall, where he was not moving, experiencing Drizzly weather, not following/close to a public transportation route, and neither going up nor down.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mohce\\llm-project\\main.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mohce/llm-project/main.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m description \u001b[39m=\u001b[39m create_context_description(c)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mohce/llm-project/main.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCURRENT: \u001b[39m\u001b[39m'\u001b[39m, description)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mohce/llm-project/main.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m most_similar_examples \u001b[39m=\u001b[39m example_manager\u001b[39m.\u001b[39;49mget_most_similar_examples(description)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mohce/llm-project/main.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m#print('SIMILAR: \\n', most_similar_examples) \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mohce/llm-project/main.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#print(description)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mohce/llm-project/main.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m prompt \u001b[39m=\u001b[39m prompt_generator\u001b[39m.\u001b[39mgenerate_prompt(description, most_similar_examples)\n",
      "File \u001b[1;32mc:\\Users\\mohce\\llm-project\\examples_manager.py:18\u001b[0m, in \u001b[0;36mExamples_manager.get_most_similar_examples\u001b[1;34m(self, context, similarity)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_most_similar_examples\u001b[39m(\u001b[39mself\u001b[39m, context, similarity \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcosine\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     17\u001b[0m     \u001b[39mif\u001b[39;00m similarity \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcosine\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 18\u001b[0m         most_similar_examples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_similarity_cosine(context, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexamples_df)\n\u001b[0;32m     19\u001b[0m     \u001b[39mif\u001b[39;00m similarity \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mjaccard\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     20\u001b[0m         most_similar_examples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_similarity_jaccard(context, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexamples_df)\n",
      "File \u001b[1;32mc:\\Users\\mohce\\llm-project\\examples_manager.py:26\u001b[0m, in \u001b[0;36mExamples_manager.text_similarity_cosine\u001b[1;34m(self, context, pool_examples, k)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtext_similarity_cosine\u001b[39m(\u001b[39mself\u001b[39m, context, pool_examples, k\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m):\n\u001b[1;32m---> 26\u001b[0m     cosine_similarities \u001b[39m=\u001b[39m [util\u001b[39m.\u001b[39mcos_sim(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_embeddings\u001b[39m.\u001b[39mencode(context, convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_embeddings\u001b[39m.\u001b[39mencode(doc, convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m pool_examples[\u001b[39m'\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m     27\u001b[0m     document_similarity_scores \u001b[39m=\u001b[39m [(i, score[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy()) \u001b[39mfor\u001b[39;00m i, score \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(cosine_similarities)]\n\u001b[0;32m     28\u001b[0m     sorted_similarity_scores \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(document_similarity_scores, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\mohce\\llm-project\\examples_manager.py:26\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtext_similarity_cosine\u001b[39m(\u001b[39mself\u001b[39m, context, pool_examples, k\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m):\n\u001b[1;32m---> 26\u001b[0m     cosine_similarities \u001b[39m=\u001b[39m [util\u001b[39m.\u001b[39mcos_sim(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_embeddings\u001b[39m.\u001b[39;49mencode(context, convert_to_tensor\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_embeddings\u001b[39m.\u001b[39mencode(doc, convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m pool_examples[\u001b[39m'\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m     27\u001b[0m     document_similarity_scores \u001b[39m=\u001b[39m [(i, score[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy()) \u001b[39mfor\u001b[39;00m i, score \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(cosine_similarities)]\n\u001b[0;32m     28\u001b[0m     sorted_similarity_scores \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(document_similarity_scores, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\mohce\\llm-project\\env\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:161\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m start_index \u001b[39min\u001b[39;00m trange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(sentences), batch_size, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatches\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n\u001b[0;32m    160\u001b[0m     sentences_batch \u001b[39m=\u001b[39m sentences_sorted[start_index:start_index\u001b[39m+\u001b[39mbatch_size]\n\u001b[1;32m--> 161\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenize(sentences_batch)\n\u001b[0;32m    162\u001b[0m     features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[0;32m    164\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\mohce\\llm-project\\env\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:319\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, texts: Union[List[\u001b[39mstr\u001b[39m], List[Dict], List[Tuple[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]]):\n\u001b[0;32m    316\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39m    Tokenizes the texts\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_first_module()\u001b[39m.\u001b[39;49mtokenize(texts)\n",
      "File \u001b[1;32mc:\\Users\\mohce\\llm-project\\env\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:113\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_lower_case:\n\u001b[0;32m    111\u001b[0m     to_tokenize \u001b[39m=\u001b[39m [[s\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m col] \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m to_tokenize]\n\u001b[1;32m--> 113\u001b[0m output\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer(\u001b[39m*\u001b[39;49mto_tokenize, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, truncation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlongest_first\u001b[39;49m\u001b[39m'\u001b[39;49m, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_seq_length))\n\u001b[0;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\mohce\\llm-project\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2790\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2788\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2789\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2790\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_one(text\u001b[39m=\u001b[39mtext, text_pair\u001b[39m=\u001b[39mtext_pair, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_kwargs)\n\u001b[0;32m   2791\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2792\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\mohce\\llm-project\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2876\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2871\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2872\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2873\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2874\u001b[0m         )\n\u001b[0;32m   2875\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[1;32m-> 2876\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2877\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2878\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2879\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2880\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   2881\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2882\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2883\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2884\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2885\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2886\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2887\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2888\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2889\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2890\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2891\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2892\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2893\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2894\u001b[0m     )\n\u001b[0;32m   2895\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2896\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2897\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2898\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2914\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2915\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mohce\\llm-project\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3067\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3058\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3059\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   3060\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3064\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   3065\u001b[0m )\n\u001b[1;32m-> 3067\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   3068\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   3069\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   3070\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m   3071\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   3072\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   3073\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   3074\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   3075\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3076\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   3077\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   3078\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   3079\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   3080\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3081\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3082\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   3083\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   3084\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   3085\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\mohce\\llm-project\\env\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:504\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[0;32m    497\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m    498\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    501\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    502\u001b[0m )\n\u001b[1;32m--> 504\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode_batch(\n\u001b[0;32m    505\u001b[0m     batch_text_or_text_pairs,\n\u001b[0;32m    506\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m    507\u001b[0m     is_pretokenized\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[0;32m    508\u001b[0m )\n\u001b[0;32m    510\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[0;32m    516\u001b[0m tokens_and_encodings \u001b[39m=\u001b[39m [\n\u001b[0;32m    517\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_encoding(\n\u001b[0;32m    518\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m encodings\n\u001b[0;32m    528\u001b[0m ]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "all_responses = []\n",
    "all_usages = []\n",
    "i = 0\n",
    "for c, l, o in list(zip(unique_contexts, unique_contexts_labels, unique_contexts_ontology_consistencies)):\n",
    "    i += 1\n",
    "    print('Index: ',i)\n",
    "    print(c)\n",
    "    description = create_context_description(c)\n",
    "    print('CURRENT: ', description)\n",
    "    most_similar_examples = example_manager.get_most_similar_examples(description)\n",
    "    #print('SIMILAR: \\n', most_similar_examples) \n",
    "    #print(description)\n",
    "    prompt = prompt_generator.generate_prompt(description, most_similar_examples)\n",
    "    #print(prompt)\n",
    "    responses, usages = request_handler.handle_request(prompt, repetitions=1, return_all=True)\n",
    "    #print(responses)\n",
    "    all_responses.append(responses)\n",
    "    all_usages.append(usages)\n",
    "    print(\"---------------------------------------------------------------------------------\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1211a64bfa7f11",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Dump data\n",
    "Increment n each time to have different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f958c1f38567b18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T08:14:08.035428300Z",
     "start_time": "2023-10-25T08:14:07.999524400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32779ea0d39528f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dump answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3d76bd96bb2592f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T08:14:14.207799500Z",
     "start_time": "2023-10-25T08:14:14.162328900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if DATASET == \"DOMINO\":\n",
    "    answers_path = f\"answers/domino/a_cosine_{n}.pkl\"\n",
    "elif DATASET == \"EXTRASENSORY\":\n",
    "    answers_path = f\"answers/extrasensory/a_{n}.pkl\"\n",
    "else:\n",
    "    answers_path = None\n",
    "    raise ValueError(\"DATASET name must be 'DOMINO' or 'EXTRASENSORY'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9757b1633f9d9a6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T08:14:14.411763200Z",
     "start_time": "2023-10-25T08:14:14.342645Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(answers_path, \"wb\") as file:\n",
    "    pkl.dump(all_responses, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeb1de3f4a3aa99",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Dump usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bbcefe82e11f2c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T08:32:39.703147500Z",
     "start_time": "2023-10-27T08:32:39.537152100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DATASET \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDOMINO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     usages_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musages/domino/u_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m DATASET \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEXTRASENSORY\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      4\u001b[0m     usages_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musages/extrasensory/u_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "if DATASET == \"DOMINO\":\n",
    "    usages_path = f\"usages/domino/u_{n}.pkl\"\n",
    "elif DATASET == \"EXTRASENSORY\":\n",
    "    usages_path = f\"usages/extrasensory/u_{n}.pkl\"\n",
    "else:\n",
    "    usages_path = None\n",
    "    raise ValueError(\"DATASET name must be 'DOMINO' or 'EXTRASENSORY'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94230f1055b2f420",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(usages_path, \"wb\") as file:\n",
    "    pkl.dump(all_usages, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac524d73909ed0c0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load answers and ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "444ad2f989868013",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T08:14:57.887047500Z",
     "start_time": "2023-10-25T08:14:57.840201Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if DATASET == \"DOMINO\":\n",
    "    answers_path = f\"answers/domino/a_{n}.pkl\"\n",
    "elif DATASET == \"EXTRASENSORY\":\n",
    "    answers_path = f\"answers/extrasensory/a_{n}.pkl\"\n",
    "else:\n",
    "    answers_path = None\n",
    "    raise ValueError(\"DATASET name must be 'DOMINO' or 'EXTRASENSORY'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6262ec3951d8633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T08:14:58.103655700Z",
     "start_time": "2023-10-25T08:14:58.054660800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if DATASET == \"DOMINO\":\n",
    "    ontology_path = f'datasets/domino/domino_unique_contexts_ontology_consistencies.pkl'\n",
    "elif DATASET == \"EXTRASENSORY\":\n",
    "    ontology_path = f\"datasets/extrasensory/extrasensory_unique_contexts_ontology_consistencies.pkl\"\n",
    "else:\n",
    "    answers_path = None\n",
    "    raise ValueError(\"DATASET name must be 'DOMINO' or 'EXTRASENSORY'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19bf7c0262f1f635",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T08:14:58.241007400Z",
     "start_time": "2023-10-25T08:14:58.175269300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#with open(answers_path, \"rb\") as file:\n",
    "#    all_responses = pkl.load(file)\n",
    "with open(ontology_path, \"rb\") as file:\n",
    "    ontology_consistencies = pkl.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0626f2d25f10d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Compute results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff58772a35144ff6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T08:14:58.977773800Z",
     "start_time": "2023-10-25T08:14:58.938883600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_occurrences(activity, list_of_lists):\n",
    "    count = 0\n",
    "    for sublist in list_of_lists:\n",
    "        count += sublist.count(activity)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15009cea0c181cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T08:14:59.182227800Z",
     "start_time": "2023-10-25T08:14:59.140339300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_classes_from_binary_vector(bin_vec):\n",
    "    classes_to_return = []\n",
    "    for v, c in zip(bin_vec, CLASSES):\n",
    "        if v==1:\n",
    "            classes_to_return.append(c)\n",
    "    return classes_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "293c22886a21f4d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T08:15:23.821583600Z",
     "start_time": "2023-10-25T08:15:23.792660400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a373895dff4b6b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T08:15:24.280790500Z",
     "start_time": "2023-10-25T08:15:24.247879100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for response in all_responses:\n",
    "    # print(response)\n",
    "    aggregated_response = []\n",
    "    for act in CLASSES:\n",
    "        if count_occurrences(act.lower(), response) >= min_count:\n",
    "            aggregated_response.append(1)\n",
    "        else:\n",
    "            aggregated_response.append(0)\n",
    "    preds.append(aggregated_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34079bbb6d6c094b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T08:14:19.327818100Z",
     "start_time": "2023-10-25T08:14:19.297858200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(ontology_consistencies, preds, target_names=CLASSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3155628260ca33",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Get results for DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "791962036abf9730",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T08:17:34.051830Z",
     "start_time": "2023-10-25T08:17:33.987953200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
